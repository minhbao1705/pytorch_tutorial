{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi4GhT32kOYk5mmWajq3fI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhbao1705/pytorch_tutorial/blob/main/PyTorch_Tutorial_How_to_Develop_Deep_Learning_Models_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. How to Install PyTorch**\n",
        "Trong bài này, bạn sẽ khám phá PyTorch là gì, cách cài đặt và cách xác nhận nó được cài đúng."
      ],
      "metadata": {
        "id": "EBVrOsaYhoeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1. What are Torch and PyTorch**\n",
        "\n"
      ],
      "metadata": {
        "id": "duiOeuv6iAc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch là thư viện open-source Python dành cho deeplearning được phát triển và duy trì bởi Facebook.\n",
        "\n",
        "Dự án bắt đầu vào năm 2016 và nhanh chóng trở thành một framework phổ biến đối với các nhà phát triển và nhà nghiên cứu.\n",
        "\n",
        "Torch (Torch7) là một dự án nguồn mở dành cho deep learning được viết bằng C và thường được sử dụng thông qua giao diện Lua. Đó là dự án tiền thân của PyTorch và không còn được phát triển tích cực nữa. PyTorch bao gồm \"Torch\" trong tên, thừa nhận thư viện đèn pin trước đó có tiền tố \"Py\" biểu thị trọng tâm Python của dự án mới.\n",
        "\n",
        "API PyTorch rất đơn giản và linh hoạt, khiến nó được các học giả và nhà nghiên cứu yêu thích trong việc phát triển các mô hình và ứng dụng deep learning mới. Việc sử dụng rộng rãi đã dẫn đến nhiều tiện ích mở rộng cho các ứng dụng cụ thể (chẳng hạn như văn bản, thị giác máy tính và dữ liệu âm thanh) và có thể các mô hình được đào tạo trước có thể được sử dụng trực tiếp. Vì vậy, nó có thể là thư viện phổ biến nhất được các học giả sử dụng.\n",
        "\n",
        "Tính linh hoạt của PyTorch phải trả giá bằng sự dễ sử dụng, đặc biệt đối với người mới bắt đầu, so với các giao diện đơn giản hơn như Keras. Lựa chọn sử dụng PyTorch thay vì Keras mang lại sự dễ sử dụng, đường cong học tập dốc hơn một chút và nhiều mã hơn để linh hoạt hơn và có lẽ là một cộng đồng học thuật sôi động hơn"
      ],
      "metadata": {
        "id": "gDGrb78ojyEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. How to Install PyTorch**"
      ],
      "metadata": {
        "id": "WFiWfAVxjumJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Có nhiều cách để tải thư viện deep learning open-source PyTorch\n",
        "\n",
        "Ví dụ: trên dòng lệnh, bạn có thể gõ:\n",
        "\n",
        "\n",
        "```python\n",
        "!sudo pip install torch\n",
        "```\n",
        "\n",
        "Có lẽ ứng dụng phổ biến nhất của deep learning là dành cho thị giác máy tính và gói thị giác máy tính PyTorch được gọi là \"torchvision\".\n",
        "\n",
        "Việc cài đặt torchvision cũng rất được khuyến khích và nó có thể được cài đặt như sau:\n",
        "\n",
        "```python\n",
        "sudo pip install torchvision\n",
        "```\n",
        "\n",
        "Bạn có thể xem danh sách đầy đủ các hướng dẫn cài đặt tại đây:\n",
        "* [PyTorch Installation Guide](https://pytorch.org/get-started/locally/)"
      ],
      "metadata": {
        "id": "HFbKKzwaj5-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3. How to confirm PyTorch is Installed**"
      ],
      "metadata": {
        "id": "f7KFMS6Ll0jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau khi cài đặt PyTorch, điều quan trọng là phải xác nhận rằng thư viện đã được cài đặt thành công và bạn có thể bắt đầu sử dụng nó."
      ],
      "metadata": {
        "id": "YBhRml1FmQE1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sofcNAv_YOYh",
        "outputId": "44da2f11-3a48-45b6-ad8e-f3c58dd14a74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ],
      "source": [
        "# check pytorch version\n",
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. PyTorch Deep Learning Model Life-Cycle**"
      ],
      "metadata": {
        "id": "xzDEexROm2c5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trong phần này, bạn sẽ khám phá vòng đời của mô hình học sâu và API PyTorch mà bạn có thể sử dụng để xác định mô hình.\n",
        "\n",
        "Một mô hình có vòng đời và kiến ​​thức rất đơn giản này cung cấp nền tảng cho cả việc lập mô hình tập dữ liệu và hiểu API PyTorch.\n",
        "Năm bước trong vòng đời như sau:\n",
        "1. Chuẩn bị dữ liệu.\n",
        "2. Xác định mô hình.\n",
        "3. Đào tạo người mẫu.\n",
        "4. Đánh giá mô hình.\n",
        "5. Đưa ra dự đoán.\n",
        "\n",
        "Chúng ta hãy xem xét kỹ hơn lần lượt từng bước."
      ],
      "metadata": {
        "id": "LuGxav0TnLkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Prepare the Data**"
      ],
      "metadata": {
        "id": "ZvzSkaJFn9fY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bước đầu tiên là tải và chuẩn bị dữ liệu của bạn.\n",
        "\n",
        "Các mô hình mạng thần kinh yêu cầu dữ liệu đầu vào dạng số và dữ liệu đầu ra dạng số.\n",
        "\n",
        "Bạn có thể sử dụng thư viện Python tiêu chuẩn để tải và chuẩn bị dữ liệu dạng bảng, như tệp CSV. Ví dụ: Pandas có thể được sử dụng để tải tệp CSV của bạn và các công cụ từ scikit-learn có thể được sử dụng để mã hóa dữ liệu phân loại, chẳng hạn như nhãn lớp.\n",
        "\n",
        "PyTorch cung cấp lớp Tập dữ liệu mà bạn có thể mở rộng và tùy chỉnh để tải tập dữ liệu của mình.\n",
        "\n",
        "Ví dụ: hàm tạo của đối tượng tập dữ liệu có thể tải tệp dữ liệu của bạn (ví dụ: tệp CSV). Sau đó, bạn có thể ghi đè hàm `__len__()` có thể dùng để lấy độ dài của tập dữ liệu (số hàng hoặc mẫu) và hàm `__getitem__()` dùng để lấy mẫu cụ thể theo chỉ mục.\n",
        "\n",
        "Khi tải tập dữ liệu của mình, bạn cũng có thể thực hiện bất kỳ phép biến đổi cần thiết nào, chẳng hạn như scaling hoặc encoding.\n"
      ],
      "metadata": {
        "id": "9dPRWUwyoEsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset definition\n",
        "class CSVDataset(Dataset):\n",
        "  # load the dataset\n",
        "  def __init(self, path):\n",
        "    # store the inputs and outputs\n",
        "    self.X = ...\n",
        "    self.y = ...\n",
        "\n",
        "  # number of rows in the dataset\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  # get a row at an index\n",
        "  def __getitem__(self, idx):\n",
        "    return [self.X[idx], self.y[idx]]"
      ],
      "metadata": {
        "id": "-i16_KpNmmZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau khi được tải, PyTorch cung cấp lớp DataLoader để điều hướng một phiên bản Dataset trong quá trình đào tạo và đánh giá mô hình của bạn.\n",
        "\n",
        "Một phiên bản DataLoader có thể được tạo cho tập training dataset, test dataset và thậm chí cả validation dataset.\n",
        "\n",
        "Hàm `Random_split()` có thể được sử dụng để chia tập data thành train và test set. Sau khi phân tách, một lựa chọn các hàng từ Dataset có thể được cung cấp cho DataLoader, cùng với batch size và liệu data có nên được xáo trộn mỗi epoch hay không.\n",
        "\n",
        "Ví dụ: chúng ta có thể xác định DataLoader bằng cách chuyển vào một mẫu hàng đã chọn trong tập dữ liệu."
      ],
      "metadata": {
        "id": "MYdiqVRRp2_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataset\n",
        "dataset = CSVDataset(...)\n",
        "# select rows from the dataset\n",
        "train, test = random_split(dataset, [[...], [...]])\n",
        "# create a data loader for train and test sets\n",
        "train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
        "test_dl = DataLoader(test, batch_size=1024, shuffle=False)"
      ],
      "metadata": {
        "id": "uOiA_HVUsRwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau khi được xác định, DataLoader có thể được liệt kê, mang lại giá trị một lô mẫu cho mỗi lần lặp."
      ],
      "metadata": {
        "id": "KhUdT-PGsYHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "for i, (inputs, targets) in enumerate(train_dl):\n",
        "..."
      ],
      "metadata": {
        "id": "JkG5-RHhsh3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Define the Model**"
      ],
      "metadata": {
        "id": "xSdlU2G2spmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bước tiếp theo là xác định một mô hình.\n",
        "\n",
        "Hàm tạo của lớp của bạn xác định các lớp của mô hình và hàm Forward() là phần ghi đè xác định cách chuyển tiếp đầu vào truyền qua các lớp được xác định của mô hình.\n",
        "\n",
        "Có nhiều lớp, chẳng hạn như Linear cho các lớp được kết nối đầy đủ, Conv2d cho các lớp chập và MaxPool2d cho các lớp gộp.\n",
        "\n",
        "Các acitvation function cũng có thể được định nghĩa là các lớp, chẳng hạn như ReLU, Softmax và Sigmoid.\n",
        "\n",
        "Dưới đây là một ví dụ về mô hình MLP đơn giản với một lớp."
      ],
      "metadata": {
        "id": "cLHLPxgSst0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model definition\n",
        "class MLP(Module):\n",
        "  # define model elements\n",
        "  def __init__(self, n_inputs):\n",
        "    super(MLP, self).__init__()\n",
        "    self.layer = Linear(n_inputs, 1)\n",
        "    self.activation = Sigmoid()\n",
        "\n",
        "  # forward propagate input\n",
        "  def forward(self, X):\n",
        "    X = self.layer(X)\n",
        "    X = self.activation(X)\n",
        "    return X"
      ],
      "metadata": {
        "id": "WfO0hwkwsslc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Các trọng số của một lớp nhất định cũng có thể được khởi tạo sau khi lớp đó được xác định trong hàm tạo.\n",
        "\n",
        "Các ví dụ phổ biến bao gồm các sơ đồ khởi tạo trọng số Xavier và He. Ví dụ:"
      ],
      "metadata": {
        "id": "V0Z4k1tGuh6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xavier_uniform_(self.layer.weight)"
      ],
      "metadata": {
        "id": "2EB_U9l0ulmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Train the Model**"
      ],
      "metadata": {
        "id": "InVCuVG_uzzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quá trình đào tạo yêu cầu bạn xác định hàm mất mát và thuật toán tối ưu hóa.\n",
        "\n",
        "Các hàm mất mát phổ biến bao gồm:\n",
        "\n",
        "* BCELoss: Mất entropy chéo nhị phân để phân loại nhị phân.\n",
        "* CrossEntropyLoss: Mất entropy chéo theo phân loại để phân loại nhiều lớp.\n",
        "* MSELoss: Tổn thất bình phương trung bình cho hồi quy.\n",
        "\n",
        "Để biết thêm về các hàm mất mát nói chung, hãy xem hướng dẫn:\n",
        "* [Hàm mất mát và mất mát để đào tạo Mạng thần kinh học sâu ](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/)\n",
        "\n",
        "Độ dốc ngẫu nhiên được sử dụng để tối ưu hóa và thuật toán tiêu chuẩn được cung cấp bởi lớp SGD, mặc dù có sẵn các phiên bản khác của thuật toán, chẳng hạn như như Adam."
      ],
      "metadata": {
        "id": "y6wjwiuKvZEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the optimization\n",
        "criterion = MSELoss()\n",
        "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "anJB3XBsu2mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huấn luyện mô hình bao gồm việc liệt kê DataLoader cho training dataset.\n",
        "\n",
        "Đầu tiên, cần có một vòng lặp cho training epochs. Sau đó, cần có một vòng lặp bên trong cho các mini-batches để gradient descent ngẫu nhiên."
      ],
      "metadata": {
        "id": "trPotttZwsLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enumerate epochs\n",
        "for epoch in range(100):\n",
        "  # enumerate mini batches\n",
        "  for i, (inputs, targets) in enumerate(train_dl):"
      ],
      "metadata": {
        "id": "kErGlGh0xDb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mỗi bản cập nhật cho mô hình đều bao gồm cùng một mẫu chung bao gồm:\n",
        "* Clearing the last error gradient.\n",
        "* A forward pass of the input through the model.\n",
        "* Calculating the loss for the model output.\n",
        "* Backpropagating the error through the model.\n",
        "* Update the model in an effort to reduce loss.\n",
        "\n",
        "Ví dụ:"
      ],
      "metadata": {
        "id": "rrKta9kmxM_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clear the gradients\n",
        "optimizer.zero_grad()\n",
        "# compute the model output\n",
        "yhat = model(inputs)\n",
        "# calculate loss\n",
        "loss = criterion(yhat, targets)\n",
        "# credit assignment\n",
        "loss.backward()\n",
        "# update model weights\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "YWta3MsQxhGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Evaluate the model**"
      ],
      "metadata": {
        "id": "_25HOfz2x20V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Khi mô hình phù hợp, nó có thể được đánh giá trên tập dữ liệu thử nghiệm.\n",
        "\n",
        "Điều này có thể đạt được bằng cách sử dụng DataLoader cho test dataset và thu thập dự đoán cho test set, sau đó so sánh dự đoán với giá trị mong đợi của test set và tính toán chỉ số hiệu suất."
      ],
      "metadata": {
        "id": "cFNHTwX1x63U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (inputs, targets) in enumerate(test_dl):\n",
        "  # evaluate the model on the test set\n",
        "  yhat = model(inputs)"
      ],
      "metadata": {
        "id": "bl43jllOx6Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Make predictions**"
      ],
      "metadata": {
        "id": "ff9kzyloy0Dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Một mô hình phù hợp có thể được sử dụng để đưa ra dự đoán về dữ liệu mới.\n",
        "\n",
        "Ví dụ: bạn có thể có một hình ảnh hoặc một hàng dữ liệu và muốn đưa ra dự đoán.\n",
        "\n",
        "Điều này yêu cầu bạn bọc dữ liệu trong cấu trúc dữ liệu PyTorch Tensor.\n",
        "\n",
        "Tensor chỉ là phiên bản PyTorch của mảng NumPy để lưu trữ dữ liệu. Nó cũng cho phép bạn thực hiện các tác vụ phân biệt tự động trong biểu đồ mô hình, như gọi `backward()` khi huấn luyện mô hình.\n",
        "\n",
        "Dự đoán cũng sẽ là một Tensor, mặc dù bạn có thể truy xuất mảng NumPy bằng cách tách Tensor khỏi biểu đồ vi phân tự động và gọi hàm NumPy"
      ],
      "metadata": {
        "id": "jYhypTxoy3Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert row to data\n",
        "row = Variable(Tensor([ros]).float())\n",
        "# make prediction\n",
        "yhat = model(row)\n",
        "# retrieve numpy array\n",
        "yhat = yhat.detach().numpy()"
      ],
      "metadata": {
        "id": "sDAGKMk-y2Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. How to Develop PyTorch Deep Learning Models**"
      ],
      "metadata": {
        "id": "rCydHq5ezwdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trong phần này, bạn sẽ khám phá cách phát triển, đánh giá và đưa ra dự đoán bằng các mô hình học sâu tiêu chuẩn, bao gồm Multilayer Perceptrons (MLP) và Convolutional Neural Networks (CNN).\n",
        "\n",
        "Mô hình Perceptron đa lớp, hay gọi tắt là MLP, là mô hình mạng thần kinh được kết nối đầy đủ tiêu chuẩn.\n",
        "\n",
        "Nó bao gồm các lớp nút trong đó mỗi nút được kết nối với tất cả đầu ra từ lớp trước và đầu ra của mỗi nút được kết nối với tất cả đầu vào của các nút ở lớp tiếp theo.\n",
        "\n",
        "MLP là một mô hình có một hoặc nhiều lớp được kết nối đầy đủ. Mô hình này phù hợp với dữ liệu dạng bảng, tức là dữ liệu được thể hiện trong bảng hoặc bảng tính với một cột cho mỗi biến và một hàng cho mỗi biến. Có ba vấn đề về mô hình dự đoán mà bạn có thể muốn khám phá với MLP; chúng là phân loại nhị phân, phân loại nhiều lớp và hồi quy.\n",
        "\n",
        "Hãy điều chỉnh mô hình trên tập dữ liệu thực cho từng trường hợp này.\n",
        "\n",
        "Lưu ý: Các mô hình trong phần này có hiệu quả nhưng chưa được tối ưu hóa. Xem liệu bạn có thể cải thiện hiệu suất của họ hay không. Đăng những phát hiện của bạn trong các ý kiến ​​​​dưới đây."
      ],
      "metadata": {
        "id": "-crYW_HHz4VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1. How to Develop an MLP for Binary Classification**"
      ],
      "metadata": {
        "id": "8dB2Ahd70b9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chúng tôi sẽ sử dụng tập dữ liệu phân loại nhị phân tầng điện ly (hai lớp) để chứng minh MLP cho phân loại nhị phân.\n",
        "\n",
        "Bộ dữ liệu này liên quan đến việc dự đoán liệu có cấu trúc nào trong khí quyển hay không hoặc radar không phản hồi.\n",
        "\n",
        "Chúng tôi sẽ sử dụng LabelEncode để mã hóa nhãn chuỗi thành giá trị số nguyên 0 và 1. Mô hình sẽ phù hợp với 67% dữ liệu và 33% còn lại sẽ được sử dụng để đánh giá, phân tách bằng hàm `train_test_split()`.\n",
        "\n",
        "Đó là một cách thực hành tốt để sử dụng kích hoạt 'relu' với khởi tạo trọng số *'He Uniform'*. Sự kết hợp này đã đi một chặng đường dài để khắc phục vấn đề biến mất độ dốc khi training deep neural network models.\n",
        "\n",
        "Mô hình dự đoán xác suất của lớp 1 và sử dụng hàm kích hoạt sigmoid. Mô hình được tối ưu hóa bằng cách sử dụng phương pháp giảm độ dốc ngẫu nhiên và tìm cách giảm thiểu tổn thất binary cross-entropy loss.\n",
        "\n",
        "Ví dụ đầy đủ được liệt kê dưới đây."
      ],
      "metadata": {
        "id": "zwdZyZ080wv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch mlp for binary classification\n",
        "from numpy import vstack\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Sigmoid\n",
        "from torch.nn import Module\n",
        "from torch.optim import SGD\n",
        "from torch.nn import BCELoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_\n",
        "\n",
        "# dataset definition\n",
        "class CSVDataset(Dataset):\n",
        "  # load the dataset\n",
        "  def __init__(self, path):\n",
        "    # load the csv file as a dataframe\n",
        "    df = read_csv(path, header=None)\n",
        "    # store the inputs and outputs\n",
        "    self.X = df.values[:, :-1]\n",
        "    self.y = df.values[:, -1]\n",
        "    # ensure input data is floats\n",
        "    self.X = self.X.astype('float32')\n",
        "    # label encode target and ensure the values are floats\n",
        "    self.y = LabelEncoder().fit_transform(self.y)\n",
        "    self.y = self.y.astype('float32')\n",
        "    self.y = self.y.reshape((len(self.y), 1))\n",
        "\n",
        "  # number of rows in the dataset\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  # get a row at an index\n",
        "  def __getitem__(self, idx):\n",
        "    return [self.X[idx], self.y[idx]]\n",
        "\n",
        "  # get indexes for train and test rows\n",
        "  def get_splits(self, n_test=0.33):\n",
        "    # determine sizes\n",
        "    test_size = round(n_test * len(self.X))\n",
        "    train_size = len(self.X) - test_size\n",
        "    # calculate the split\n",
        "    return random_split(self, [train_size, test_size])\n",
        "\n",
        "# model definition\n",
        "class MLP(Module):\n",
        "  # define model elements\n",
        "  def __init__(self, n_inputs):\n",
        "    super(MLP, self).__init__()\n",
        "    # input to first hidden layer\n",
        "    self.hidden1 = Linear(n_inputs, 10)\n",
        "    kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
        "    self.act1 = ReLU()\n",
        "    # second hidden layer\n",
        "    self.hidden2 = Linear(10, 8)\n",
        "    kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
        "    self.act2 = ReLU()\n",
        "    # third hidden layer and output\n",
        "    self.hidden3 = Linear(8, 1)\n",
        "    xavier_uniform_(self.hidden3.weight)\n",
        "    self.act3 = Sigmoid()\n",
        "\n",
        "  # forward propagate input\n",
        "  def forward(self, X):\n",
        "    # input to first hidden layer\n",
        "    X = self.hidden1(X)\n",
        "    X = self.act1(X)\n",
        "    # second hidden layer\n",
        "    X = self.hidden2(X)\n",
        "    x = self.act2(X)\n",
        "    # third hidden layer and output\n",
        "    X = self.hidden3(X)\n",
        "    X = self.act3(X)\n",
        "    return X\n",
        "\n",
        "# prepare the dataset\n",
        "def prepare_data(path):\n",
        "  # load the dataset\n",
        "  dataset = CSVDataset(path)\n",
        "  # calculate split\n",
        "  train, test = dataset.get_splits()\n",
        "  # prepare data loaders\n",
        "  train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
        "  test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
        "  return train_dl, test_dl\n",
        "\n",
        "# train the model\n",
        "def train_model(train_dl, model):\n",
        "  # define the optimization\n",
        "  criterion = BCELoss()\n",
        "  optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "  # enumerate epochs\n",
        "  for epoch in range(100):\n",
        "    # enumerate mini batches\n",
        "    for i, (inputs, targets) in enumerate(train_dl):\n",
        "      # clear the gradients\n",
        "      optimizer.zero_grad()\n",
        "      # compute the model output\n",
        "      yhat = model(inputs)\n",
        "      # calculate loss\n",
        "      loss = criterion(yhat, targets)\n",
        "      # credict assignment\n",
        "      loss.backward()\n",
        "      # update model weights\n",
        "      optimizer.step()\n",
        "\n",
        "# evaluate the model\n",
        "def evaluate_model(test_dl, model):\n",
        "  predictions, actuals = list(), list()\n",
        "  for i, (inputs, targets) in enumerate(test_dl):\n",
        "    # evaluate the model on the test set\n",
        "    yhat = model(inputs)\n",
        "    # retrieve numpy array\n",
        "    yhat = yhat.detach().numpy()\n",
        "    actual = targets.numpy()\n",
        "    actual = actual.reshape((len(actual), 1))\n",
        "    # round to class values\n",
        "    yhat = yhat.round()\n",
        "    # store\n",
        "    predictions.append(yhat)\n",
        "    actuals.append(actual)\n",
        "  predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "  # calculate accuracy\n",
        "  acc = accuracy_score(actuals, predictions)\n",
        "  return acc\n",
        "\n",
        "# make a class prediction for one row of data\n",
        "def predict(row, model):\n",
        "  # convert row to data\n",
        "  row = Tensor([row])\n",
        "  # make prediction\n",
        "  yhat = model(row)\n",
        "  # retrieve numpy array\n",
        "  yhat = yhat.detach().numpy()\n",
        "  return yhat\n",
        "\n",
        "# prepare the data\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
        "train_dl, test_dl = prepare_data(path)\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))\n",
        "# define the network\n",
        "model = MLP(34)\n",
        "# train the model\n",
        "train_model(train_dl, model)\n",
        "# evaluate the model\n",
        "acc = evaluate_model(test_dl, model)\n",
        "print('Accuracy: %3f' % acc)\n",
        "# make a single prediction (expect class=1)\n",
        "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
        "yhat = predict(row, model)\n",
        "print('Predicted: %3f (class=%3d)' % (yhat, yhat.round()))"
      ],
      "metadata": {
        "id": "CvAeVDX1z001",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d24b36-1d7d-48cb-9dae-3e78aed2d62f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "235 116\n",
            "Accuracy: 0.948276\n",
            "Predicted: 0.999283 (class=  1)\n"
          ]
        }
      ]
    }
  ]
}