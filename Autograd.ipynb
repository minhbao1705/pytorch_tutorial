{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIvjbeXxAmE4wf5buLOMXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhbao1705/pytorch_tutorial/blob/main/Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trong phần này sẽ tìm hiểu về cách tính đạo hàm bằng cách sử dụng autograd package trong Pytorch"
      ],
      "metadata": {
        "id": "Qd6ZYdQHQF-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 - The Autograd package**\n",
        "\n",
        "Autograd package cung cấp một sự khác biệt tự động cho tất cả các hoạt động của tensor. Rất dễ dàng để sử dụng đạo hàm trong pytorch bằng cách chỉ cho nó biết rằng tensor cần được đạo hàm bằng requires_grad. Với việc thiết lập thuộc tính này, các phép toán trên tensor đều được theo dõi trên một đồ thị tính toán."
      ],
      "metadata": {
        "id": "E97xxMlYQO9A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwZSgsovP2no"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "y = x + 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  - ở cell trên y đã được tạo ra bởi kết quả của phép tính x + 2, vì vậy nó sẽ tạo một thuộc tính grad_fn.\n",
        "  - grad_fn: tham chiếu đến một hàm đã tạo tensor\n",
        "\"\"\"\n",
        "print(x) # Đã được tạo ở cell trên -> grad_fn lúc này là None\n",
        "print(y)\n",
        "print(y.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fb-z1WQ6cw",
        "outputId": "f1dc01c9-057e-427b-be21-02282b371510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.7603, -1.2176,  0.3986], requires_grad=True)\n",
            "tensor([3.7603, 0.7824, 2.3986], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x7fca61376d40>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện các phép tính khác trên y\n",
        "z = y * 3\n",
        "print(z)\n",
        "z = z + 3\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQPByp1TRQoi",
        "outputId": "bb28648c-ce04-4558-87ae-85071d3d3009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([11.2808,  2.3471,  7.1957], grad_fn=<MulBackward0>)\n",
            "tensor([14.2808,  5.3471, 10.1957], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 - Tính đạo hàm với lan truyền ngược**\n",
        "\n",
        "Khi hoàn tất quá trình tính toán, ta có thể gọi .backward() và tất cả giá trị đạo hàm sẽ được tính toán một cách tự động. Giá trị đạo hàm của những tensor này sẽ được tích lũy vào trong thuộc tính.grad. Nó chính là đạo hàm riêng của tensor."
      ],
      "metadata": {
        "id": "_9VIntL9Sw5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = z.mean()\n",
        "z.backward()\n",
        "print(x.grad) # dz/dx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTlF1sJeR4K3",
        "outputId": "a746d520-eac0-4a4a-9177-fa96d7c090c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(5, requires_grad=True)\n",
        "print(x)\n",
        "y = x*2\n",
        "for _ in range(10):\n",
        "  y = y * 2\n",
        "\n",
        "print(y)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRVFprrqTm5g",
        "outputId": "dcf7e179-2f25-4d2c-9569-0af5d61c1293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.8002,  0.4505,  0.2386,  0.1756,  2.1282], requires_grad=True)\n",
            "tensor([-3686.8625,   922.6259,   488.6931,   359.5541,  4358.5483],\n",
            "       grad_fn=<MulBackward0>)\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor([0.1, 1.0, 0.0001, 0.01, 0.001], dtype=torch.float32)\n",
        "y.backward(w)\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZMlPx8HUzVk",
        "outputId": "bcdac000-70a5-4786-e02e-1a753f35aeea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01, 2.0480e+01, 2.0480e+00])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 - Stop a tensor from tracking history**"
      ],
      "metadata": {
        "id": "RC5AQyd1VN_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trong quá trình huấn luyện, khi chúng ta muốn cập nhật trọng số thì thao tác cập nhật này không nên là một phần của phép tính đạo hàm. Chúng ta có 3 sự lựa chọn cho việc dừng quá trình đạo hàm và cập nhật tham số như sau:\n",
        "- x.requires_grad_false()\n",
        "- x.detach()\n",
        "- wrap in with torch.no_grad():"
      ],
      "metadata": {
        "id": "PrsuxN1BVUvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**.requires_grad_(...) thay đổi yêu cầu ngay tại vị trí cần yêu cầu đạo hàm**"
      ],
      "metadata": {
        "id": "yXduKUoQVXg4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfFibb31VAUm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}